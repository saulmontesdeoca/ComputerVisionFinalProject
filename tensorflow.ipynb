{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2 as cv2\n",
    "from tensorflow.keras import models, layers \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "\n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# load the DNN model\n",
    "mobile_net_model = cv2.dnn.readNet(model='models/frozen_inference_graph.pb',\n",
    "                        config='models/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt', \n",
    "                        framework='TensorFlow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrameWithBoxes(img):\n",
    "    copy_image = img.copy()\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    # create blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123))\n",
    "    # create blob from image\n",
    "    mobile_net_model.setInput(blob)\n",
    "    # forward pass through the mobile_net_model to carry out the detection\n",
    "    output = mobile_net_model.forward()\n",
    "\n",
    "    # loop over each of the detection\n",
    "    for detection in output[0, 0, :, :]:\n",
    "        # extract the confidence of the detection\n",
    "        confidence = detection[2]\n",
    "        # draw bounding boxes only if the detection confidence is above...\n",
    "        # ... a certain threshold, else skip\n",
    "        if confidence > .7:\n",
    "            # get the class id\n",
    "            class_id = detection[1]\n",
    "            # map the class id to the class\n",
    "            class_name = class_names[int(class_id)-1]\n",
    "            color = COLORS[int(class_id)]\n",
    "            # get the bounding box coordinates\n",
    "            box_x = detection[3] * image_width\n",
    "            box_y = detection[4] * image_height\n",
    "            # get the bounding box width and height\n",
    "            box_width = detection[5] * image_width\n",
    "            box_height = detection[6] * image_height\n",
    "            # draw a rectangle around each detected object\n",
    "            cv2.rectangle(copy_image, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=2)\n",
    "            # put the FPS text on top of the frame\n",
    "            cv2.putText(copy_image, class_name, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    return copy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('input/video_1.mp4')\n",
    "\n",
    "# get the video frames' width and height for proper saving of videos\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# create the `VideoWriter()` object\n",
    "out = cv2.VideoWriter('output/video_result_1.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_RGB(img):\n",
    "    # convert to YUV color space\n",
    "    red = img[:,:,0]\n",
    "    green = img[:,:,1]\n",
    "    blue = img[:,:,2]\n",
    "    # equalize the red, green, and blue channels\n",
    "    red_eq = cv2.equalizeHist(red)\n",
    "    green_eq = cv2.equalizeHist(green)\n",
    "    blue_eq = cv2.equalizeHist(blue)\n",
    "    # merge the channels back together\n",
    "    img_eq = cv2.merge((red_eq, green_eq, blue_eq))\n",
    "    return img_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# detect objects in each frame of the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        image_eq = equalize_RGB(frame)\n",
    "        image_boxes = getFrameWithBoxes(image_eq)\n",
    "\n",
    "        cv2.imshow('image', image_boxes)\n",
    "        out.write(image_boxes)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
